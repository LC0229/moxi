# Moxi ğŸš€

**AI-Powered Documentation Generator with Continuous Learning**

Moxi is an intelligent documentation generator that analyzes GitHub repositories and creates high-quality, comprehensive documentation using fine-tuned Large Language Models. Inspired by [llm-twin-course](https://github.com/decodingml/llm-twin-course), Moxi implements a complete ML pipeline from data collection to model deployment.

---

## ğŸ¯ What Does Moxi Do?

Moxi automates the tedious process of writing documentation by:
1. **Analyzing** your codebase structure and key files
2. **Understanding** your project's architecture and purpose
3. **Generating** professional README files, architecture docs, and API documentation
4. **Learning** from thousands of high-quality open-source projects

---

## âœ¨ Key Features

- ğŸ¤– **Custom Fine-Tuned Model** - Train your own Llama-3.1-8B specialized for documentation
- ğŸ“Š **Automated Dataset Generation** - Create training data from 10,000+ GitHub repositories
- ğŸ” **Intelligent Repository Analysis** - Parse and understand project structures
- ğŸ“ **Multi-Format Documentation** - Generate README, ARCHITECTURE, API docs
- ğŸ¯ **A/B Testing** - Compare your model against GPT-4
- ğŸ³ **Docker Ready** - Easy deployment with Docker Compose
- ğŸ› ï¸ **CLI Tool** - Simple command-line interface

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MOXI PIPELINE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£  REPO ANALYZER
    â”œâ”€â”€ GitHub Crawler
    â”œâ”€â”€ Structure Parser
    â””â”€â”€ File Analyzer
    
2ï¸âƒ£  DATASET GENERATOR
    â”œâ”€â”€ GitHub Trending Crawler
    â”œâ”€â”€ Instruction Generator (GPT-4)
    â””â”€â”€ Quality Control
    
3ï¸âƒ£  TRAINING PIPELINE
    â”œâ”€â”€ SFT Trainer (LoRA/QLoRA)
    â”œâ”€â”€ Experiment Tracking (W&B)
    â””â”€â”€ Model Evaluation
    
4ï¸âƒ£  DOC GENERATOR
    â”œâ”€â”€ Custom Model Inference
    â”œâ”€â”€ Format & Validate
    â””â”€â”€ Export Markdown
    
5ï¸âƒ£  CLI & EVALUATION
    â”œâ”€â”€ Command-Line Interface
    â””â”€â”€ A/B Testing Framework
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- OpenAI API Key (for dataset generation)
- Hugging Face Token (for model training)
- GitHub Token (for repo crawling)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/moxi.git
cd moxi

# Create virtual environment
python -m venv moxi
source moxi/bin/activate  # On Windows: moxi\Scripts\activate

# Install dependencies
make install

# Copy and configure environment variables
cp .env.example .env
# Edit .env with your API keys
```

### Basic Usage

```bash
# Test configuration
make test-config

# Analyze a repository
make local-analyze-repo REPO=https://github.com/pytorch/pytorch

# Generate documentation (using pre-trained model)
make local-generate-docs REPO=https://github.com/pytorch/pytorch
```

---

## ğŸ“š Full Pipeline

### Step 1: Generate Training Dataset

```bash
# Crawl 100+ high-quality GitHub repositories
make crawl-github-repos

# Generate 10,000 training samples using GPT-4
make generate-training-dataset

# Validate dataset quality
make validate-dataset
```

**Cost:** ~$20-30 in OpenAI API credits

### Step 2: Train Your Model

```bash
# Download base Llama-3.1-8B model
make download-base-model

# Train with Supervised Fine-Tuning (SFT)
make train-sft

# Evaluate model performance
make evaluate-model
```

**Time:** 4-8 hours on GPU (AWS g5.2xlarge recommended)

### Step 3: Generate Documentation

```bash
# Use your trained model
make local-generate-docs REPO=https://github.com/user/repo

# Compare with GPT-4 baseline
make compare-models
```

---

## ğŸ› ï¸ Development

### Project Structure

```
moxi/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/               # Configuration, logging, utilities
â”‚   â”œâ”€â”€ repo_analyzer/      # GitHub crawler & parser
â”‚   â”œâ”€â”€ dataset_generator/  # Training data creation
â”‚   â”œâ”€â”€ training_pipeline/  # Model training & evaluation
â”‚   â”œâ”€â”€ doc_generator/      # Documentation generation
â”‚   â””â”€â”€ cli/                # Command-line interface
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ data/                   # Downloaded repositories
â”œâ”€â”€ training_data/          # Generated datasets
â”œâ”€â”€ models/                 # Trained models
â””â”€â”€ Makefile               # All commands
```

### Running Tests

```bash
# Run all tests
make test

# Run specific test suites
make test-unit
make test-integration
```

### Code Quality

```bash
# Lint code
make lint

# Format code
make format
```

---

## ğŸ“Š Dataset & Training

### Dataset Statistics

- **Source:** GitHub repositories (100+ stars)
- **Size:** 10,000+ instruction-output pairs
- **Format:** JSON with instruction, input, output fields
- **Quality:** Validated by GPT-4 + human review

### Training Configuration

- **Base Model:** Meta Llama-3.1-8B-Instruct
- **Method:** Supervised Fine-Tuning (SFT) with LoRA
- **Hyperparameters:**
  - Learning Rate: 2e-4
  - Batch Size: 4 (per device)
  - Gradient Accumulation: 4 steps
  - Epochs: 3
  - Max Sequence Length: 2048 tokens

### Hardware Requirements

- **Training:** GPU with 16GB+ VRAM (NVIDIA A10G, V100, or better)
- **Inference:** GPU with 8GB+ VRAM or CPU (slower)

---

## ğŸ³ Docker Deployment

```bash
# Build containers
make docker-build

# Start services
make docker-up

# View logs
make docker-logs

# Stop services
make docker-down
```

---

## ğŸ“– CLI Usage

```bash
# Generate documentation
python -m cli.main generate https://github.com/user/repo

# Analyze repository structure
python -m cli.main analyze https://github.com/user/repo

# Evaluate model performance
python -m cli.main evaluate --compare custom vs gpt4
```

---

## ğŸ¯ Roadmap

- [x] Phase 0: Project Setup
- [x] Phase 1: Core Infrastructure
- [ ] Phase 2: Repository Analyzer
- [ ] Phase 3: Dataset Generator
- [ ] Phase 4: Training Pipeline
- [ ] Phase 5: Documentation Generator
- [ ] Phase 6: CLI & Evaluation
- [ ] Phase 7: Docker & Deployment
- [ ] Phase 8: Documentation & Demo

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Inspired by [llm-twin-course](https://github.com/decodingml/llm-twin-course) by Paul Iusztin
- Built with [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)
- Powered by Hugging Face Transformers, PEFT, and TRL

---

## ğŸ“§ Contact

- **Author:** Shengrui Chen
- **Email:** chenleon572@gmail.com
- **GitHub:** [@yourusername](https://github.com/yourusername)

---

â­ **Star this repo if you find it useful!** â­

